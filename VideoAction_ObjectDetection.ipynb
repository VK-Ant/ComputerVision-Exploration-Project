{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48e6eab418cd491ea56c147eb97356f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_043058973e48420398c28976c67ce680",
              "IPY_MODEL_51d55a348aec43f0a9840ea2fe6842f2",
              "IPY_MODEL_17e9ccd58a104c51b6ade86bd43a25d8"
            ],
            "layout": "IPY_MODEL_92ba044f9d214d1db4295640684252f9"
          }
        },
        "043058973e48420398c28976c67ce680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581d424f9354461facaf63b3dd278cbc",
            "placeholder": "​",
            "style": "IPY_MODEL_ea61fe796bc44468ad2ef069fdd7eee2",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "51d55a348aec43f0a9840ea2fe6842f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_868e4176c8724973bac392dd7e4109d3",
            "max": 605143284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0506aa7667c042f19e01bd398c5ab33e",
            "value": 605143284
          }
        },
        "17e9ccd58a104c51b6ade86bd43a25d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17aa54908dca4214a44b3b9f90e5e72a",
            "placeholder": "​",
            "style": "IPY_MODEL_b9e53a1858d14b64925ebb56ea30d894",
            "value": " 605M/605M [00:08&lt;00:00, 49.2MB/s]"
          }
        },
        "92ba044f9d214d1db4295640684252f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581d424f9354461facaf63b3dd278cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea61fe796bc44468ad2ef069fdd7eee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "868e4176c8724973bac392dd7e4109d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0506aa7667c042f19e01bd398c5ab33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17aa54908dca4214a44b3b9f90e5e72a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e53a1858d14b64925ebb56ea30d894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **VIDEO ACTION & OBJECT DETECTION -> MULTIMODAL**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "DATE: 29 NOV 2025\n"
      ],
      "metadata": {
        "id": "lbbYz7xDylJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U open-clip-torch --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ0lsjqLeE8Q",
        "outputId": "a4e8fddb-2366-404a-b590-a22eb18dd603"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.24.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2025.11.3)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.7.0)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (1.0.22)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open-clip-torch) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.11.12)\n",
            "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m322.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, open-clip-torch\n",
            "Successfully installed ftfy-6.3.1 open-clip-torch-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtLSy3SPk7dW",
        "outputId": "fe43b484-cb2a-486b-c8d2-55ab2623ffc9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.233-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.233-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.233 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\", pretrained=\"openai\"\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"CLIP model loaded on:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "48e6eab418cd491ea56c147eb97356f5",
            "043058973e48420398c28976c67ce680",
            "51d55a348aec43f0a9840ea2fe6842f2",
            "17e9ccd58a104c51b6ade86bd43a25d8",
            "92ba044f9d214d1db4295640684252f9",
            "581d424f9354461facaf63b3dd278cbc",
            "ea61fe796bc44468ad2ef069fdd7eee2",
            "868e4176c8724973bac392dd7e4109d3",
            "0506aa7667c042f19e01bd398c5ab33e",
            "17aa54908dca4214a44b3b9f90e5e72a",
            "b9e53a1858d14b64925ebb56ea30d894"
          ]
        },
        "id": "Rutdf967eHs2",
        "outputId": "4f9e53fd-e8a3-4b0e-b922-a34bd663b32c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48e6eab418cd491ea56c147eb97356f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP model loaded on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import open_clip\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# =============================\n",
        "# CONFIG\n",
        "# =============================\n",
        "input_video = \"/content/drive/MyDrive/yolo/car.mp4\"\n",
        "output_video = \"/content/output_yolo_clip_race_car.mp4\"\n",
        "output_csv   = \"/content/output_yolo_clip_race_car.csv\"\n",
        "\n",
        "frame_skip = 3\n",
        "motion_boost_weight = 0.8\n",
        "smooth_window = 5\n",
        "conf_yolo = 0.25\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# =============================\n",
        "# LOAD YOLOv8\n",
        "# =============================\n",
        "yolo_model = YOLO(\"yolov8n.pt\")   # nano = fastest for Colab free GPU\n",
        "\n",
        "# =============================\n",
        "# LOAD CLIP\n",
        "# =============================\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\", pretrained=\"openai\"\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# =============================\n",
        "# PROMPTS (HORSE + CAR)\n",
        "# =============================\n",
        "texts = [\n",
        "    \"a professional horse racing competition\",\n",
        "    \"horses sprinting on a racetrack with jockeys\",\n",
        "    \"a jockey riding a racing horse\",\n",
        "    \"a horse galloping at high speed\",\n",
        "\n",
        "    \"a professional car racing competition\",\n",
        "    \"race cars speeding on a racetrack\",\n",
        "    \"a formula car racing at high speed\",\n",
        "    \"multiple cars competing in a race\",\n",
        "\n",
        "    \"a person walking\",\n",
        "    \"a person running\",\n",
        "    \"a person standing\"\n",
        "]\n",
        "\n",
        "horse_label_indices = [0, 1, 2, 3]\n",
        "car_label_indices   = [4, 5, 6, 7]\n",
        "\n",
        "text_tokens = tokenizer(texts).to(device)\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# =============================\n",
        "# VIDEO SETUP\n",
        "# =============================\n",
        "cap = cv2.VideoCapture(input_video)\n",
        "if not cap.isOpened():\n",
        "    raise ValueError(\"Cannot open input video\")\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "\n",
        "# =============================\n",
        "# PROCESS VIDEO\n",
        "# =============================\n",
        "rows = []\n",
        "frame_features = []\n",
        "motion_magnitudes = []\n",
        "\n",
        "prev_gray = None\n",
        "frame_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "print(\"Processing video...\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # ----------------------------------\n",
        "    # YOLO DETECTION\n",
        "    # ----------------------------------\n",
        "    results = yolo_model(frame, conf=conf_yolo, verbose=False)[0]\n",
        "\n",
        "    crops = []\n",
        "    boxes_draw = []\n",
        "\n",
        "    for box in results.boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        conf = float(box.conf[0])\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "        class_name = yolo_model.names[cls_id]\n",
        "\n",
        "        # Only keep horses + cars\n",
        "        if class_name in [\"horse\", \"car\"]:\n",
        "            crop = frame[y1:y2, x1:x2]\n",
        "            if crop.size > 0:\n",
        "                crops.append(crop)\n",
        "                boxes_draw.append((x1, y1, x2, y2, class_name, conf))\n",
        "\n",
        "    # ----------------------------------\n",
        "    # MOTION (OPTICAL FLOW)\n",
        "    # ----------------------------------\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    if prev_gray is None:\n",
        "        flow_mag = 0.0\n",
        "    else:\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, gray, None,\n",
        "            0.5, 3, 15, 3, 5, 1.2, 0\n",
        "        )\n",
        "        mag, _ = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
        "        flow_mag = float(np.mean(mag))\n",
        "\n",
        "    prev_gray = gray.copy()\n",
        "    motion_magnitudes.append(flow_mag)\n",
        "\n",
        "    # ----------------------------------\n",
        "    # CLIP ON CROPS\n",
        "    # ----------------------------------\n",
        "    clip_scores = []\n",
        "\n",
        "    for crop in crops:\n",
        "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(crop_rgb)\n",
        "        image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_feat = model.encode_image(image)\n",
        "            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "            sim = (img_feat @ text_features.T).squeeze(0).cpu().numpy()\n",
        "            sim_soft = np.exp(sim) / np.sum(np.exp(sim) + 1e-8)\n",
        "            clip_scores.append(sim_soft)\n",
        "\n",
        "    # Average across all detected crops\n",
        "    if len(clip_scores) > 0:\n",
        "        clip_scores = np.mean(np.array(clip_scores), axis=0)\n",
        "    else:\n",
        "        clip_scores = np.ones(len(texts)) / len(texts)\n",
        "\n",
        "    frame_features.append(clip_scores)\n",
        "\n",
        "    # ----------------------------------\n",
        "    # TEMPORAL + MOTION BOOST\n",
        "    # ----------------------------------\n",
        "    K = min(len(frame_features), smooth_window)\n",
        "    avg_clip = np.mean(np.array(frame_features[-K:]), axis=0)\n",
        "\n",
        "    recent_motion = np.array(motion_magnitudes[-K:])\n",
        "    p95 = np.percentile(motion_magnitudes, 95) if len(motion_magnitudes) > 1 else 1.0\n",
        "    motion_norm = float(np.clip(np.mean(recent_motion) / (p95 + 1e-6), 0, 1))\n",
        "\n",
        "    boosted = avg_clip.copy()\n",
        "\n",
        "    for idx in horse_label_indices:\n",
        "        boosted[idx] *= (1.0 + motion_boost_weight * motion_norm)\n",
        "\n",
        "    for idx in car_label_indices:\n",
        "        boosted[idx] *= (1.0 + motion_boost_weight * motion_norm)\n",
        "\n",
        "    boosted = boosted / (np.sum(boosted) + 1e-8)\n",
        "\n",
        "    pred_idx = int(np.argmax(boosted))\n",
        "    pred_label = texts[pred_idx]\n",
        "    pred_conf = float(boosted[pred_idx])\n",
        "\n",
        "    # ----------------------------------\n",
        "    # DRAW YOLO BOXES\n",
        "    # ----------------------------------\n",
        "    draw_frame = frame.copy()\n",
        "\n",
        "    for (x1, y1, x2, y2, cls, conf) in boxes_draw:\n",
        "        cv2.rectangle(draw_frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(\n",
        "            draw_frame, f\"{cls} {conf:.2f}\",\n",
        "            (x1, y1-5),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.6, (0,255,0), 2\n",
        "        )\n",
        "\n",
        "    # ----------------------------------\n",
        "    # DRAW ACTION\n",
        "    # ----------------------------------\n",
        "    cv2.rectangle(draw_frame, (0,0), (width,80), (0,0,0), -1)\n",
        "\n",
        "    cv2.putText(\n",
        "        draw_frame,\n",
        "        f\"Action: {pred_label} ({pred_conf:.2f})\",\n",
        "        (20,40),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1.0, (0,255,255), 2\n",
        "    )\n",
        "\n",
        "    cv2.putText(\n",
        "        draw_frame,\n",
        "        f\"Motion: {motion_norm:.2f}\",\n",
        "        (20,70),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        0.7, (200,200,0), 2\n",
        "    )\n",
        "\n",
        "    out.write(draw_frame)\n",
        "\n",
        "    # ----------------------------------\n",
        "    # LOG CSV\n",
        "    # ----------------------------------\n",
        "    row = {\n",
        "        \"frame_idx\": frame_idx,\n",
        "        \"motion\": motion_norm,\n",
        "        \"pred_label\": pred_label,\n",
        "        \"confidence\": pred_conf\n",
        "    }\n",
        "\n",
        "    for i, t in enumerate(texts):\n",
        "        row[f\"clip_{i}\"] = float(boosted[i])\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# =============================\n",
        "# SAVE CSV\n",
        "# =============================\n",
        "df = pd.DataFrame(rows)\n",
        "if os.path.exists(output_csv):\n",
        "    base, ext = os.path.splitext(output_csv)\n",
        "    output_csv = f\"{base}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{ext}\"\n",
        "\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(\"\\n✅ PROCESS COMPLETED\")\n",
        "print(\"✅ Output video:\", output_video)\n",
        "print(\"✅ CSV log:\", output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtEFn2rlkmPZ",
        "outputId": "b6a524ba-b599-4361-d50a-18a3517ced4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Processing video...\n",
            "\n",
            "✅ PROCESS COMPLETED\n",
            "✅ Output video: /content/output_yolo_clip_race_car.mp4\n",
            "✅ CSV log: /content/output_yolo_clip_race_car.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uD4Ye8YPlTjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBlg3sN7jYUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}